# -*- coding: utf-8 -*-
"""C√≥pia de stopwords-frequencia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZ5M_pK7-urAVsJm3D4Q_m8_dvb2mENg

Nesta aula, vamos aprender como trabalhar com ngramas e stopwords, utilizando a bilioteca NLTK.

# NGRAMAS, Stopwords e NLTK
### Autor: Lucas Ferro Antunes de Oliveira
#### HAILab - PPGTS - PUCPR

lucas.ferro.2000@hotmail.com

#Tokeniza√ß√£o e normaliza√ß√£o do corpus
"""

# prompt: converta um arquivo pdf em txt padr√£o utf-8

# !pip install PyPDF2

import PyPDF2
import re

def convert_pdf_to_txt(pdf_path, txt_path):
    try:
        with open(pdf_path, 'rb') as pdf_file:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ''
            for page in pdf_reader.pages:
                text += page.extract_text()

            # Remove extra whitespace and newlines
            text = re.sub(r'\s+', ' ', text)

            with open(txt_path, 'w', encoding='utf-8') as txt_file:
                txt_file.write(text)

        print(f"PDF '{pdf_path}' converted to '{txt_path}' successfully.")
    except FileNotFoundError:
        print(f"Error: File '{pdf_path}' not found.")
    except Exception as e:
        print(f"An error occurred: {e}")


# Transforma .pdf em .txt
file = "aRevolucaoDosBichos"
PATH = "content/aRevolucaoDosBichos/"
reader = PyPDF2.PdfReader(""+PATH+""+file+".pdf")
qt = 0
last = 0

with open(PATH+"txt_cap1.txt",'w',encoding="utf-8") as txt_file:
    for page_num in range(len(reader.pages)):
        page = reader.pages[page_num]
        text = page.extract_text()
        # verifica passagem de cap√≠tulo
        block = ["1","1","1","1"]
        result = ""
        ardb = open(PATH+"ardb.txt","a",encoding="utf-8")
        # ardb.write(text) # +"\n\n\n\n\n ------------------------------------------------------------------------ \n\n\n\n\n"
        for char in text:
            block[0] = block[1]
            block[1] = block[2]
            block[2] = block[3]
            block[3] = char
            # qt+=1
            # if (qt >= 14491) :
            # ardb.write(f"{block}\n")
            for num in range(1,11):
                if ((block[1] == f"{num}" and block[2] == "." and block[3] == "\n") or (block[1] == "1" and block[2] == "0" and block[3] == "." and num==10)):
                    if last>num:
                        break
                        pass
                    print(f"cap {num}: {block} : {last}")
                    last = num
                    txt_file = open(PATH+f"txt_cap{num}.txt","w",encoding="utf-8")
        txt_file.write(text)
#


# Example usage:
pdf_file_path = PATH+file+'.pdf'  # Replace with your PDF file path
txt_file_path = PATH+file+'output.txt'      # Replace with desired output file path

convert_pdf_to_txt(pdf_file_path, txt_file_path)

import nltk

# Instala√ß√£o do NLTK
# !pip install nltk==3.6.2

# Importa√ß√£o de bibliotecas
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from datetime import datetime
from collections import Counter
from nltk import ngrams
import string
from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator
import matplotlib.pyplot as plt
from PIL import Image
import re
nltk.download('punkt_tab')

# pd.set_option('max_columns', None)
# pd.set_option('max_colwidth', None)

# Pega todas as pontua√ß√µes
remove_pt = string.punctuation
remove_pt

# Baixa as stopwords para o portugu√™s no NLTK
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words_pt = set(stopwords.words('portuguese'))
len(stop_words_pt)
stop_words_pt

stop_words_pt.add('ser')
stop_words_pt.add('entao')
stop_words_pt.add('de')

stop_words_pt

stop_words_pt.add('ainda')
stop_words_pt.add('por√©m')
stop_words_pt

# from google.colab import drive
# drive.mount('/content/drive')
qt_palavras = 100
num = 2 # cap√≠tulo do livro

with open(PATH + f'txt_cap{num}.txt', 'r', encoding='utf8') as f:
    filecontent = f.read()

print(filecontent[0:])

type(filecontent)

len(filecontent) # n√∫mero de tokens

"""## Transformando o texto completo em senten√ßas (tokenizer do NLTK)"""

sentencas = []
for sentence in sent_tokenize(filecontent, language = 'portuguese'):
    sentencas.append(sentence)

sentencas

index = 1
for sentenca in sentencas[0:qt_palavras]: # mostrando as 100 primeiras
    print(f'{index}: {sentenca}')
    index+=1

"""## Segmenta√ß√£o por quebra de linha e depois pelo tokenizer do NLTK"""

sentencas_linha = []
for sentence in filecontent.split('\n'):
    if sentence != '':
        for processed_sentence in sent_tokenize(sentence, language = 'portuguese'):
            sentencas_linha.append(processed_sentence)

index = 1
for sentenca in sentencas_linha[0:qt_palavras]: # mostrando as 100 primeiras
    print(f'{index}: {sentenca}')
    index+=1

"""## Tokeniza√ß√£o de cada senten√ßa em palavras (tokenizer do NLTK)




"""

sentencas_tokenizadas = []

for sentenca in sentencas_linha:
    tokenized_sentence = word_tokenize(sentenca, language='portuguese')
    sentencas_tokenizadas.append(tokenized_sentence)
index = 1
for tokens in sentencas_tokenizadas[0:qt_palavras]: # mostrando as 100 primeiras
    print(f'{index}: {tokens}')
    index+=1

"""## Pre-processamento dos elementos tokenizados
A ideia aqui √© retirar todas as palavras que pertencem a lista de stopwords, deixar tudo em min√∫sculos, retirar espa√ßos e quebras de linhas adicionais desnecess√°rios.
"""

from typing import TextIO
sent_tokenizada_preprocessed = []
for sent_tokenizada in sentencas_tokenizadas:
    raw = [token.lower() for token in sent_tokenizada]

    raw = [''.join(c for c in s if c not in remove_pt+'‚Äì'+'üôÅ'+'\‚Äô'+'\‚Äù'+"‚Äú") for s in raw]
    raw = [re.sub(r"\d+[.,]?\d*","", s) for s in raw]
    raw = [s for s in raw if s not in stop_words_pt] # stopwords
    raw = [' '.join(s.split()) for s in raw if s]
    string = ' '.join(raw).rstrip().lstrip()
    if string != '':
        sent_tokenizada_preprocessed.append(string)

index = 1
for texto in sent_tokenizada_preprocessed[0:qt_palavras]: # mostrando as qt_palavras primeiras
    print(f'{index}: {texto}')
    index+=1

"""#NGramas"""

print(f"tokenizados: {len(sent_tokenizada_preprocessed)}")

import os
ngram_value = 1 # valor do ngrama
most_common_value = qt_palavras

ngram_counts = [list(ngrams(s.split(), ngram_value)) for s in sent_tokenizada_preprocessed]
flat_ngram_counts = [item for sublist in ngram_counts for item in sublist]
ngram_list = Counter(flat_ngram_counts)

common = ngram_list.most_common(most_common_value)

df_common = pd.DataFrame(common, columns = ['Ngram','Count'])
index = 1
for n_gram in ngram_counts[0:most_common_value]: # mostrando as qt_palavras primeiras
    print(f'{index}: {n_gram}')
    index+=1

index = 1
for n_gram in flat_ngram_counts: # mostrando as qt_palavras primeiras
    print(f'{index}: {n_gram}')
    index+=1

len(ngram_list)

common

print(df_common)

# Quantidade de palavras
len(flat_ngram_counts)

# Quantidade de palavras √∫nicas
len(ngram_list)

color = '#000000'
height = 400
width = 800
max_words = 2000
colormap = 'rainbow'
size_X = 50
size_Y = 50

str_text=" ".join(sent_tokenizada_preprocessed)


wordcloud = WordCloud(background_color = color, max_words = max_words, max_font_size = 90, colormap = colormap, height = height, width = width).generate(str_text)

X = size_X/2.54
Y = size_Y/2.25

fig = plt.figure(figsize = [X, Y])
plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis("off")
plt.box(False)
plt.show()

"""fazer a frequencia, Carlos"""