{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hiZVkjn822t"
      },
      "source": [
        "Nesta aula, vamos aprender como trabalhar com ngramas e stopwords, utilizando a bilioteca NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3zkx7T73M5"
      },
      "source": [
        "# NGRAMAS, Stopwords e NLTK\n",
        "### Autor: Lucas Ferro Antunes de Oliveira\n",
        "#### HAILab - PPGTS - PUCPR\n",
        "\n",
        "lucas.ferro.2000@hotmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42YvY7xA-PpC"
      },
      "source": [
        "#Tokeniza√ß√£o e normaliza√ß√£o do corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZhget0XeU9L",
        "outputId": "54794b26-e811-454b-dc6c-465e25a3ef73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m This environment is externally managed\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\n",
            "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
            "\u001b[31m   \u001b[0m install.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
            "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
            "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
            "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
            "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
            "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
            "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{sys.executable}\u001b[39;00m\u001b[38;5;124m -m pip install PyPDF2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# !apt install python3-PyPDF2\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_pdf_to_txt\u001b[39m(pdf_path, txt_path):\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
          ]
        }
      ],
      "source": [
        "# prompt: converta um arquivo pdf em txt padr√£o utf-8\n",
        "import sys\n",
        "!{sys.executable} -m pip install PyPDF2\n",
        "\n",
        "# !apt install python3-PyPDF2\n",
        "\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "def convert_pdf_to_txt(pdf_path, txt_path):\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            text = ''\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "\n",
        "            # Remove extra whitespace and newlines\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "            with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "                txt_file.write(text)\n",
        "\n",
        "        print(f\"PDF '{pdf_path}' converted to '{txt_path}' successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{pdf_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "pdf_file_path = '/content/sample_data/ your_pdf_file.pdf'  # Replace with your PDF file path\n",
        "txt_file_path = 'output.txt'      # Replace with desired output file path\n",
        "\n",
        "convert_pdf_to_txt(pdf_file_path, txt_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuuBHqPCO0sy"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Wkn-uW7s-RYa",
        "outputId": "bfa9515c-00e8-4e9a-89c7-b5898a4c5c75"
      },
      "outputs": [],
      "source": [
        "# Instala√ß√£o do NLTK\n",
        "# !pip install nltk==3.6.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSXZ4bVf8HAH",
        "outputId": "2f122df3-3dc5-4df6-e3c9-29a1f8ed3357"
      },
      "outputs": [],
      "source": [
        "# Importa√ß√£o de bibliotecas\n",
        "# !pip install wordcloud\n",
        "# !pip install PIL\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "import string\n",
        "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import re\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('punkt_tab', force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ydiU8zL8HGW"
      },
      "outputs": [],
      "source": [
        "# Transforma .pdf em .txt\n",
        "file = \"eraDosExtremos\"\n",
        "caps = 1\n",
        "PATH = \"content/\"\n",
        "reader = PyPDF2.PdfReader(\"\"+PATH+\"\"+file+\".pdf\")\n",
        "qt = 0\n",
        "last = 0\n",
        "\n",
        "with open(PATH+file+\"_txt_cap1.txt\",'w',encoding=\"utf-8\") as txt_file:\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        page = reader.pages[page_num]\n",
        "        text = page.extract_text()\n",
        "        # verifica passagem de cap√≠tulo\n",
        "        block = [\"1\",\"1\",\"1\",\"1\"]\n",
        "        result = \"\"\n",
        "        ardb = open(PATH+\"ardb.txt\",\"a\",encoding=\"utf-8\")\n",
        "        # ardb.write(text) # +\"\\n\\n\\n\\n\\n ------------------------------------------------------------------------ \\n\\n\\n\\n\\n\"\n",
        "        for char in text:\n",
        "            block[0] = block[1]\n",
        "            block[1] = block[2]\n",
        "            block[2] = block[3]\n",
        "            block[3] = char\n",
        "            qt+=1\n",
        "            if (qt >= 148734) :\n",
        "                ardb.write(f\"{block}\\n\")\n",
        "            # for num in range(1,11):\n",
        "            #     if ((block[1] == f\"{num}\" and block[2] == \".\" and block[3] == \"\\n\") or (block[1] == \"1\" and block[2] == \"0\" and block[3] == \".\" and num==10)):\n",
        "            #         if last>num:\n",
        "            #             break\n",
        "            #             pass\n",
        "            #         print(f\"cap {num}: {block} : {last}\")\n",
        "            #         last = num\n",
        "            #         # txt_file = open(PATH+f\"{file}_txt_cap{num}.txt\",\"w\",encoding=\"utf-8\")\n",
        "            for num in range(1,11):\n",
        "                if ((block[0] == f\"{num}\" and block[1] == \"\\n\" ) or (block[1] == \"1\" and block[2] == f\"{num%10}\" and block[3] == \"\\n\")):\n",
        "                    if last>=num:\n",
        "                        break\n",
        "                        pass\n",
        "                    print(f\"cap {num}: {block} : {last} {qt}\")\n",
        "                    last = num\n",
        "                    # txt_file = open(PATH+f\"{file}_txt_cap{num}.txt\",\"w\",encoding=\"utf-8\")\n",
        "        txt_file.write(text)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UV21w1US8HJG",
        "outputId": "eb8c14a9-19ba-4059-9f6f-ea0aa0c23546"
      },
      "outputs": [],
      "source": [
        "# Pega todas as pontua√ß√µes\n",
        "remove_pt = string.punctuation\n",
        "remove_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjd4QLA88HLe",
        "outputId": "73c05e62-0c00-441c-985a-1e160f30ecdf"
      },
      "outputs": [],
      "source": [
        "# Baixa as stopwords para o portugu√™s no NLTK\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_pt = set(stopwords.words('portuguese'))\n",
        "len(stop_words_pt)\n",
        "stop_words_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87i9Qfc685w3",
        "outputId": "6267cbd7-39b3-4794-f20f-cf63112cad4a"
      },
      "outputs": [],
      "source": [
        "stop_words_pt.add('ser')\n",
        "stop_words_pt.add('entao')\n",
        "stop_words_pt.add('de')\n",
        "\n",
        "stop_words_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0c9Kokp8HN-",
        "outputId": "aeb3940f-06e2-453f-8a1f-6208708ae0d7"
      },
      "outputs": [],
      "source": [
        "stop_words_pt.add('ainda')\n",
        "stop_words_pt.add('por√©m')\n",
        "stop_words_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPGXCql5o093"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrI_IoO79UEP"
      },
      "outputs": [],
      "source": [
        "PATH = 'content/'\n",
        "for num in range(1,caps+1):\n",
        "    print(f\"Cap√≠tulo {num}:\")\n",
        "    with open(PATH + f'{file}_txt_cap{num}.txt', 'r', encoding='utf8') as f:\n",
        "        filecontent = f.read()\n",
        "\n",
        "    # print(filecontent[0:])\n",
        "    type(filecontent)\n",
        "    len(filecontent) # n√∫mero de tokens\n",
        "    ## Transformando o texto completo em senten√ßas (tokenizer do NLTK)\n",
        "    sentencas = []  \n",
        "    for sentence in sent_tokenize(filecontent, language='english'):\n",
        "        sentencas.append(sentence)\n",
        "    # sentencas\n",
        "    index = 1\n",
        "    # for sentenca in sentencas[0:100]: # mostrando as 100 primeiras\n",
        "        # print(f'{index}: {sentenca}')\n",
        "        # index+=1\n",
        "    ## Segmenta√ß√£o por quebra de linha e depois pelo tokenizer do NLTK\n",
        "    sentencas_linha = []\n",
        "    for sentence in filecontent.split('\\n'):\n",
        "        if sentence != '':\n",
        "            for processed_sentence in sent_tokenize(sentence, language = 'portuguese'):\n",
        "                sentencas_linha.append(processed_sentence)\n",
        "    index = 1\n",
        "    # for sentenca in sentencas_linha[0:100]: # mostrando as 100 primeiras\n",
        "        # print(f'{index}: {sentenca}')\n",
        "        # index+=1\n",
        "    ## Tokeniza√ß√£o de cada senten√ßa em palavras (tokenizer do NLTK)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    sentencas_tokenizadas = []\n",
        "\n",
        "    for sentenca in sentencas_linha:\n",
        "        tokenized_sentence = word_tokenize(sentenca, language='portuguese')\n",
        "        sentencas_tokenizadas.append(tokenized_sentence)\n",
        "    index = 1\n",
        "    # for tokens in sentencas_tokenizadas[0:100]: # mostrando as 100 primeiras\n",
        "        # print(f'{index}: {tokens}')\n",
        "        # index+=1\n",
        "    ## Pre-processamento dos elementos tokenizados\n",
        "    # A ideia aqui √© retirar todas as palavras que pertencem a lista de stopwords, deixar tudo em min√∫sculos, retirar espa√ßos e quebras de linhas adicionais desnecess√°rios.\n",
        "    from typing import TextIO\n",
        "    sent_tokenizada_preprocessed = []\n",
        "    for sent_tokenizada in sentencas_tokenizadas:\n",
        "        raw = [token.lower() for token in sent_tokenizada]\n",
        "\n",
        "        raw = [''.join(c for c in s if c not in remove_pt+'‚Äì'+'üôÅ'+'\\‚Äô'+'\\‚Äù'+\"‚Äú\") for s in raw]\n",
        "        raw = [re.sub(r\"\\d+[.,]?\\d*\",\"\", s) for s in raw]\n",
        "        raw = [s for s in raw if s not in stop_words_pt] # stopwords\n",
        "        raw = [' '.join(s.split()) for s in raw if s]\n",
        "        string = ' '.join(raw).rstrip().lstrip()\n",
        "        if string != '':\n",
        "            sent_tokenizada_preprocessed.append(string)\n",
        "\n",
        "    index = 1\n",
        "    # for texto in sent_tokenizada_preprocessed[0:100]: # mostrando as 100 primeiras\n",
        "        # print(f'{index}: {texto}')\n",
        "        # index+=1\n",
        "\n",
        "    #NGramas\n",
        "    len(sent_tokenizada_preprocessed)\n",
        "    import os\n",
        "    ngram_value = 1\n",
        "    most_common_value = 100\n",
        "\n",
        "    ngram_counts = [list(ngrams(s.split(), ngram_value)) for s in sent_tokenizada_preprocessed]\n",
        "    flat_ngram_counts = [item for sublist in ngram_counts for item in sublist]\n",
        "    ngram_list = Counter(flat_ngram_counts)\n",
        "\n",
        "    common = ngram_list.most_common(most_common_value)\n",
        "\n",
        "    df_common = pd.DataFrame(common, columns = ['Ngram','Count'])\n",
        "    # index = 1\n",
        "    # for n_gram in ngram_counts[0:100]: # mostrando as 100 primeiras\n",
        "    #     print(f'{index}: {n_gram}')\n",
        "    #     index+=1\n",
        "    # index = 1\n",
        "    # for n_gram in flat_ngram_counts[0:100]: # mostrando as 100 primeiras\n",
        "    #     print(f'{index}: {n_gram}')\n",
        "    #     index+=1\n",
        "    len(ngram_list)\n",
        "    # common\n",
        "    df_common.head(30)\n",
        "    # Quantidade de palavras\n",
        "    len(flat_ngram_counts)\n",
        "    # Quantidade de palavras √∫nicas\n",
        "    len(ngram_list)\n",
        "    color = 'black'\n",
        "    height = 400\n",
        "    width = 800\n",
        "    max_words = 2000\n",
        "    colormap = 'rainbow'\n",
        "    size_X = 50\n",
        "    size_Y = 50\n",
        "\n",
        "    str_text=\" \".join(sent_tokenizada_preprocessed)\n",
        "\n",
        "\n",
        "    wordcloud = WordCloud(background_color = color, max_words = max_words, max_font_size = 90, colormap = colormap, height = height, width = width).generate(str_text)\n",
        "\n",
        "    X = size_X/2.54\n",
        "    Y = size_Y/2.25\n",
        "\n",
        "    fig = plt.figure(figsize = [X, Y])\n",
        "    plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.box(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkYVN95Q5D7I"
      },
      "source": [
        "fazer a frequencia, Carlos"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
