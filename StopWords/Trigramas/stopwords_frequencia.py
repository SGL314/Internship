# -*- coding: utf-8 -*-
"""C√≥pia de stopwords-frequencia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZ5M_pK7-urAVsJm3D4Q_m8_dvb2mENg

Nesta aula, vamos aprender como trabalhar com ngramas e stopwords, utilizando a bilioteca NLTK.

# NGRAMAS, Stopwords e NLTK
### Autor: Lucas Ferro Antunes de Oliveira
#### HAILab - PPGTS - PUCPR

lucas.ferro.2000@hotmail.com

#Tokeniza√ß√£o e normaliza√ß√£o do corpus
"""

import PyPDF2
import re
import nltk
import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import Counter
from nltk import ngrams
import string
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords

def convert_pdf_to_txt(pdf_path, txt_path):
    try:
        with open(pdf_path, 'rb') as pdf_file:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ''
            for page in pdf_reader.pages:
                text += page.extract_text()

            # Remove extra whitespace and newlines
            text = re.sub(r'\s+', ' ', text)

            with open(txt_path, 'w', encoding='utf-8') as txt_file:
                txt_file.write(text)

        print(f"PDF '{pdf_path}' converted to '{txt_path}' successfully.")
    except FileNotFoundError:
        print(f"Erro: O arquivo '{pdf_path}' n√£o foi encontrado.")
    except Exception as e:
        print(f"Ocorreu um erro: {e}")


# Transforma .pdf em .txt
file = "aRevolucaoDosBichos"
PATH = "content/aRevolucaoDosBichos/"

# Garante que o diret√≥rio 'content' existe
import os
os.makedirs(PATH, exist_ok=True)

pdf_file_path = PATH + file + '.pdf'
txt_file_path_full = PATH + file + '.txt'

# Cria um arquivo PDF dummy para demonstra√ß√£o se o original n√£o existir
try:
    with open(pdf_file_path, 'rb') as f:
        pass
except FileNotFoundError:
    print(f"'{pdf_file_path}' n√£o encontrado. Criando um PDF dummy para demonstra√ß√£o.")
    from reportlab.pdfgen import canvas
    c = canvas.Canvas(pdf_file_path)
    c.drawString(100, 750, "Este √© um texto de exemplo para o cap√≠tulo 1.")
    c.drawString(100, 730, "Os animais estavam cansados do sr. Jones. Eles queriam liberdade.")
    c.drawString(100, 710, "Napole√£o e Bola-de-Neve assumiram a lideran√ßa da fazenda.")
    c.showPage()
    c.drawString(100, 750, "Cap√≠tulo 2.")
    c.drawString(100, 730, "A vida na fazenda dos animais era dif√≠cil, mas justa.")
    c.save()


# convert_pdf_to_txt(pdf_file_path, txt_file_path_full)


nltk.download('punkt') # Download 'punkt' para tokeniza√ß√£o
nltk.download('stopwords')

# Pega todas as pontua√ß√µes
remove_pt = string.punctuation

# Baixa as stopwords para o portugu√™s no NLTK
stop_words_pt = set(stopwords.words('portuguese'))

# Adiciona stopwords espec√≠ficas conforme seu c√≥digo original
stop_words_pt.add('ser')
stop_words_pt.add('entao')
stop_words_pt.add('de')
stop_words_pt.add('ainda')
stop_words_pt.add('por√©m')


qt_palavras = 100 # Esta vari√°vel controla o n√∫mero de itens exibidos nas listas.

# Usa o arquivo de texto completo convertido para processamento
try:
    with open(txt_file_path_full, 'r', encoding='utf8') as f:
        filecontent = f.read()
except FileNotFoundError:
    print(f"Erro: N√£o foi poss√≠vel abrir '{txt_file_path_full}'. Certifique-se de que a convers√£o do PDF foi bem-sucedida e o arquivo existe.")
    filecontent = "" # Inicializa vazio para evitar erros posteriores


print(f"Tamanho do conte√∫do: {len(filecontent)}")

"""## Transformando o texto completo em senten√ßas (tokenizer do NLTK)"""

sentencas = []
for sentence in sent_tokenize(filecontent, language = 'portuguese'):
    sentencas.append(sentence)

"""## Segmenta√ß√£o por quebra de linha e depois pelo tokenizer do NLTK"""

sentencas_linha = []
for sentence in filecontent.split('\n'):
    if sentence != '':
        for processed_sentence in sent_tokenize(sentence, language = 'portuguese'):
            sentencas_linha.append(processed_sentence)

"""## Tokeniza√ß√£o de cada senten√ßa em palavras (tokenizer do NLTK)"""

sentencas_tokenizadas = []

for sentenca in sentencas_linha:
    tokenized_sentence = word_tokenize(sentenca, language='portuguese')
    sentencas_tokenizadas.append(tokenized_sentence)

"""## Pr√©-processamento dos elementos tokenizados
A ideia aqui √© retirar todas as palavras que pertencem √† lista de stopwords, deixar tudo em min√∫sculas, retirar espa√ßos e quebras de linhas adicionais desnecess√°rios.
"""

sent_tokenizada_preprocessed = []
for sent_tokenizada in sentencas_tokenizadas:
    raw = [token.lower() for token in sent_tokenizada]

    # Remove pontua√ß√£o, caracteres espec√≠ficos e n√∫meros
    raw = [''.join(c for c in s if c not in remove_pt + '‚Äì' + 'üôÅ' + '\‚Äô' + '\‚Äù' + '‚Äú') for s in raw]
    raw = [re.sub(r"\d+[.,]?\d*", "", s) for s in raw] # Remove n√∫meros
    raw = [s for s in raw if s not in stop_words_pt] # Remove stopwords
    raw = [' '.join(s.split()) for s in raw if s] # Remove espa√ßos extras e mant√©m strings n√£o vazias
    string_processed = ' '.join(raw).strip() # Usa strip() para remover espa√ßos no in√≠cio e fim
    if string_processed: # Adiciona apenas se n√£o for uma string vazia
        sent_tokenizada_preprocessed.append(string_processed)

"""# N-gramas"""

ngram_value = 3 # Definido como 2 para bigramas
most_common_value = qt_palavras # N√∫mero dos bigramas mais comuns a serem mostrados

ngram_counts = [list(ngrams(s.split(), ngram_value)) for s in sent_tokenizada_preprocessed]
flat_ngram_counts = [item for sublist in ngram_counts for item in sublist]
ngram_list = Counter(flat_ngram_counts)

common = ngram_list.most_common(most_common_value)

# Formata os bigramas para ficarem juntos, unidos por um underscore
formatted_common = [("_".join(bigram), count) for bigram, count in common] # retira os que n√£o s√£o trigramas

df_common = pd.DataFrame(formatted_common, columns=['Trigrama', 'Frequ√™ncia'])

# Exibe apenas os bigramas e suas frequ√™ncias
print("\n--- Trigramas e suas Frequ√™ncias ---")
print(df_common)

# Nuvem de palavras
color = '#000000'
height = 400
width = 800
max_words = 2000
colormap = 'rainbow'
size_X = 50
size_Y = 50

# Para a nuvem de palavras dos bigramas, unimos os bigramas com um underscore
str_text_bigrams = " ".join(["_".join(gram) for gram in flat_ngram_counts])

wordcloud_bigrams = WordCloud(background_color = color, max_words = max_words,
                              max_font_size = 90, colormap = colormap,
                              height = height, width = width).generate(str_text_bigrams)

fig_bigrams = plt.figure(figsize = [size_X/2.54, size_Y/2.25])
plt.imshow(wordcloud_bigrams, interpolation = "bilinear")
plt.axis("off")
plt.box(False)
plt.title("Nuvem de Palavras dos Trigramas")
plt.show()